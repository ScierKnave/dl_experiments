model:
  nb_layers: 2
  nb_heads: 2
  batch_size: 32
  vocab_size: 50257
  token_size: 512
  symbolic_length: 100
  hidden_length: 100
  gradient_horizon: 7

training:
  epochs: 1
  sub_sq_length: 400
  batch_size: 32
  step_freq: 100
  model_save_freq: 20

data:
  file: reccurent_transformer/shakespear.txt
  mode: r
  encoding: utf-8
